---
title: "Image Supervised Classification with Random Forest in R"
description: |
  Here I demonstrate how to load and visualize georeferenced spatial data along with a supervised random forest classification.
  This is a work in progress and should be finished in the upcoming days.
categories:
  - Machine Learning 
  - GIS
author:
  - name: Yuri Souza
date: 2025-01-05
preview: RF_example.png
output:
  distill::distill_article:
    self_contained: false

---

I will divide the workflow of this tutorial into four main steps:

1. Getting the data read
2. Train Model
3. Evaluate Model
4. Classify Image

The data used in this tutorial can be accessed [HERE](https://github.com/souzayuri/souzayuri.github.io/tree/main/_posts/2025-01-05-random_forest_sup_classific_r)

The first thing we are gonna do is to load the required packages.
We will use *raster* and *terra* to deal with raster data, *sf* for spatial vectors data, *tidyverse* for tabulate date and visualization, *caret* for random forest, and *mapview* along with *leaflet* for data viz.


```{r packages, size = "tiny", include=TRUE}

if (!require("tidyverse")) install.packages("tidyverse")
if (!require("terra")) install.packages("terra")
if (!require("raster")) install.packages("raster")
if (!require("sf")) install.packages("sf")
if (!require("sp")) install.packages("sp")
if (!require("caret")) install.packages("caret")
if (!require("mapview")) install.packages("mapview")
if (!require("leaflet")) install.packages("leaflet")
if (!require("ggridges")) install.packages("ggridges")
if (!require("randomForest")) install.packages("randomForest")
if (!require("tmap")) install.packages("tmap")


```

Now, lets load our raster data and shapefile containing a few features delimited for use as training data in the random forest model.

```{r loading and reading data, size = "small", include=TRUE}

raw_raster <- terra::rast("woody_savanna.tif") |> 
  terra::aggregate(fact=2) # original pixel size = 10.8cm, new pixel size 21.6cm (old * 2)
raw_raster

plot(raw_raster)

sampled_area <- terra::vect("sampled_area.shp")
sampled_area
plot(sampled_area)


train_features <- sf::st_read("train_features.shp")
train_features

train_features_clnd <- sf::st_read("train_features.shp") |> 
  dplyr::rename(OBJECTID = "Classcode",
                Class = "Classname") |> 
  dplyr::mutate(OBJECTID = as.character(1:46),
                Class = as.factor(Class),
                Code = as.numeric(Class)) |> 
  dplyr::select(c(1:3))

train_features_clnd
plot(train_features_clnd)


# Compare CRS
if (terra::crs(raw_raster) == terra::crs(train_features_clnd)) {
  print("CRS are identical!")
} else {
  print("CRS are different. Therefore, reprojecting to match raster projection...")
  # Reproject train_features to match raw_raster
  train_features_clnd <- sf::st_transform(train_features_clnd, crs = terra::crs(raw_raster))
}


# Compare CRS
if (terra::crs(raw_raster) == terra::crs(train_features_clnd)) {
  print("CRS are identical!")
} else {
  print("CRS are different. Therefore, reprojecting to match raster projection...")
  # Reproject train_features to match raw_raster
  train_features_clnd <- sf::st_transform(train_features_clnd, crs = terra::crs(raw_raster))
}



```
```{r data structure, size = "small", include=TRUE}
glimpse(train_features_clnd)
summary(train_features_clnd)

```


Now, lets crop the raster and check projection

```{r cropping data, size = "small", include=TRUE}

cropped_raster <- terra::crop(raw_raster, sampled_area)
cropped_raster
plot(cropped_raster)


# Compare CRS
if (terra::crs(raw_raster) == terra::crs(train_features_clnd)) {
  print("CRS are identical!")
} else {
  print("CRS are different. Therefore, reprojecting to match raster projection...")
  # Reproject train_features to match raw_raster
  train_features_clnd <- terra::project(train_features_clnd, crs(raw_raster))
}


```
Now, lets rename our band composition

```{r defining band names, size = "small", include=TRUE}

names(cropped_raster) <- c("Blue","Green","Red","RE","NIR","MASK")
plot(cropped_raster)

```
Lets remove the MASK band as we will not use it

```{r setup, selecting bands, size = "small", include=TRUE}
cropped_raster_bands <- cropped_raster[[1:5]]
plot(cropped_raster_bands)

```
Lets make a RGB band compostion and visualize it in an interactive map with viewmap

```{r visualizing data, echo = TRUE, include=TRUE}

train_class_colors <- unique(train_features_clnd$Class) |> 
  as.data.frame() |>
  dplyr::mutate(hex = c(grass = "#ff7f00",
                        bare_soil = "#e41a1c",
                        tree = "#55eed1"))
train_class_colors

mapview::viewRGB(brick(cropped_raster_bands[[1:3]]), 
                 r = 3, g = 2, b = 1,
                 map.types = c("Esri.WorldImagery", "OpenStreetMap", "OpenTopoMap"),
                 maxpixels =  866760,
                 layer.name = "RBG Image") + 
  mapview::mapView(train_features_clnd, 
                   zcol = "Class", 
                   color = "black",  
                   lwd = 2,
                   col.regions = train_class_colors$hex,
                   layer.name = "Features")


```

Now that we have checked and named the raster bands and visualized the raster overlapped with the features that will be used for training and testing the RF model, lets do an additional raster calculation to improve the model quality in identifying features. In this step we will create a map containing the calculations of the NDVI, EVI, and SAVI index using the band composition of our raster image.

```{r visualizing datab, echo = TRUE, include=TRUE}

cropped_raster_bands_ndvi <- (cropped_raster_bands[[5]] - cropped_raster_bands[[3]]) / 
  (cropped_raster_bands[[5]] + cropped_raster_bands[[3]])
cropped_raster_bands_ndvi

cropped_raster_bands_evi <- 2.5 * ((cropped_raster_bands[[5]] - cropped_raster_bands[[3]]) /
    (cropped_raster_bands[[5]] + 6 * cropped_raster_bands[[3]] - 7.5 * cropped_raster_bands[[1]] + 1))
cropped_raster_bands_evi

cropped_raster_bands_savi <- ((cropped_raster_bands[[5]] - cropped_raster_bands[[3]]) /
    (cropped_raster_bands[[5]] + cropped_raster_bands[[3]] + 0.5)) * (1 + 0.5)
cropped_raster_bands_savi

# Create a red-to-green color palette
red_to_green_palette <- colorRampPalette(c("darkred", "yellow", "darkgreen"))
# Lets visuallize them together
par(mfrow = c(1, 3))

plot(cropped_raster_bands_ndvi, main = "NDVI", col = red_to_green_palette(100))
plot(cropped_raster_bands_evi, main = "EVI", col = red_to_green_palette(100))
plot(cropped_raster_bands_savi, main = "SAVI", col = red_to_green_palette(100))

par(mfrow = c(1, 1))


```
Let's stack these three indexes in our raster data so we can have more information to feed the model. After stacking the rasters containing the indexes with our main raster, we will notice that the new rasters are named as NIR. That is because the first spectral band we used in the above calculations was the NIR band. To avoid confusion between the NIR spectral band and the new rasters indexes, we will name them according to which index they represent.

```{r visualizing datax, echo = TRUE, include=TRUE}

train_features_clnd_indexes <- c(cropped_raster_bands, 
                                 cropped_raster_bands_ndvi,
                                 cropped_raster_bands_evi, 
                                 cropped_raster_bands_savi)
train_features_clnd_indexes


names(train_features_clnd_indexes) <- c(names(cropped_raster_bands),"NDVI", "EVI", "SAVI")
train_features_clnd_indexes

plot(train_features_clnd_indexes)

```


Now we have everything to move forward to prepare the data that will be use for training and testing our RF model. 
Lets get started by extracting the pixel values for those features we mapped. These pixel values will be used to train and test the model later.

```{r visualizing datas, echo = TRUE, include=TRUE}

train_features_clnd_pxls <- extract(train_features_clnd_indexes, train_features_clnd)
head(train_features_clnd_pxls)


train_features_clnd_pxls$Class <- train_features_clnd$Class[train_features_clnd_pxls$ID]
head(train_features_clnd_pxls)

train_features_clnd_pxls$ID <- NULL

summary(train_features_clnd_pxls)

```
Now that we have given those polygons delimitation pixel information, lets take a look at some of these pixels distribution

```{r visualizing indexes histogram, echo = TRUE, include=TRUE}
val_grass <- subset(train_features_clnd_pxls, Class == "grass")
head(val_grass)
val_trees <- subset(train_features_clnd_pxls, Class == "trees")
head(val_trees)
val_bare_soil <- subset(train_features_clnd_pxls, Class == "bare_soil")
head(val_bare_soil)

# NDVI
par(mfrow = c(3, 1))
hist(val_grass$NDVI, main = "grass", xlab = "NDVI", xlim = c(0, 1), col = "#e5ca81")
hist(val_trees$NDVI, main = "trees", xlab = "NDVI", xlim = c(0, 1), col = "#81a581")
hist(val_bare_soil$NDVI, main = "bare_soil", xlab = "NDVI", xlim = c(0, 1), col = "#7b6f89")
```


```{r visualizing indexes effects, echo = TRUE, include=TRUE}
par(mfrow = c(1, 1))

# Scatterplot and trend lines of NIR and SAVI bands 
plot(NDVI ~ EVI, data = val_grass, pch = ".", col = "#e5ca81", xlab = "SAVI", ylab = "NDVI")
points(NDVI ~ EVI, data = val_trees, pch = ".", col = "#81a581")
points(NDVI ~ EVI, data = val_bare_soil, pch = ".", col = "#7b6f89")


model_grass <- lm(NDVI ~ EVI, data = val_grass) 
abline(model_grass, col = "#e5ca81", lwd = 2)    

model_trees <- lm(NDVI ~ EVI, data = val_trees) 
abline(model_trees, col = "#81a581", lwd = 2)   


model_bare_soil <- lm(NDVI ~ EVI, data = val_bare_soil) 
abline(model_bare_soil, col = "#7b6f89", lwd = 2)        

legend("topright", legend = c("grass", "trees", "bare_soil"), 
       fill = c("#e5ca81", "#7b6f89", "#81a581"), bg = "white")


```
Lets visualize the three indexes for each mapped feature in a single graph

```{r visualizing and comparing indexes distribution using histograms, echo = TRUE, include=TRUE}

train_features_clnd_pxls_indexes <- train_features_clnd_pxls |> 
  dplyr::select(c(NDVI, EVI, SAVI, Class)) |> 
  tidyr::pivot_longer(cols = 1:3, values_to = "values", names_to = "indexes")
train_features_clnd_pxls_indexes


# add the distribution curve for each indexes parameter using stat_function and dnorm function
ggplot(train_features_clnd_pxls_indexes, 
                        aes(x = values, 
                            fill = indexes)) +
  geom_density_ridges(aes(y = 0, fill = indexes, color = indexes),  
                      jittered_points = TRUE, 
                      alpha = .5, 
                      scale = 1,
                      point_shape = "|",
                      point_size = 3, 
                      position = ggridges::position_points_jitter(width = 0.05, height = 0)) +  
    geom_vline(data = train_features_clnd_pxls_indexes |> 
               dplyr::group_by(Class, indexes) |> 
               dplyr::summarise(mean = mean(values), .groups = "drop") |> 
               dplyr::ungroup(),
             aes(xintercept = mean, color = indexes), linetype = "dashed", size = 0.8) +
  scale_fill_manual(values = c("#e5ca81", "#7b6f89", "#81a581"), labels = c("EVI", "NDVI", "SAVI")) +
  scale_color_manual(values = c("#e5ca81", "#7b6f89", "#81a581"), labels = c("EVI", "NDVI", "SAVI")) +
  theme_bw() + labs(x = "", y = "Frequency") +
  theme(axis.title = element_text(size = 20, face = "bold", color = "gray20"),
        axis.text.x.bottom = element_text(size = 14),
        axis.text.y = element_text(size = 14),
        legend.direction = "horizontal",
        legend.position = "bottom",
        legend.title = element_text(size = 16, face = "bold"),
        legend.text = element_text(size = 14),
        title = element_text(size = 18)) +
  facet_wrap(~Class, ncol = 3, scales = "free")


```

Now lets build the train and test data. At this stage we partition our dataset into two, thats a train and test dataset. 
We train the model with the train dataset and evaluate the model perfomance using the test data.

We would use the caret function createDataPartition() to split data into training and testing sets.

```{r partitioning the data, echo = TRUE, include=TRUE}
set.seed(124) 

str(train_features_clnd_pxls)

# partitioning the data into 80%
rf_train_features_clnd_pxls_indexes <- caret::createDataPartition(train_features_clnd_pxls$Class, list = FALSE, p = 0.8)
```

Now that we have partitioned 80% of the dataset, we will select that portion of that data to be used as train data (80%) and test data (20%)
If we check the number of rows in the new datasets and use the "rule of three" equation we can confirm that the data was properly split just as asked. 

```{r extracting the data to be used as train and test, echo = TRUE, include=TRUE}
# setting 80% for training - Use rf_train_features_clnd_pxls_indexes as a vector of row indices that specify which rows to extract from the train_features_clnd_pxls_indexes dataset.
rf_train_data <- train_features_clnd_pxls[rf_train_features_clnd_pxls_indexes, ] 
str(rf_train_data)

# setting 20% for training - Use rf_train_features_clnd_pxls_indexes as a vector of row indices that specify which rows to extract from the train_features_clnd_pxls_indexes dataset.
rf_test_data <- train_features_clnd_pxls[-rf_train_features_clnd_pxls_indexes, ]
str(rf_test_data)

```


Lets check the proportion of features selected by each set of data

```{r training features, echo = TRUE, include=TRUE}
rf_train_data_classCount <- rf_train_data |> 
  dplyr::group_by(Class) |> 
  count()

rf_train_data_classCount

```


```{r training feature proportions, echo = TRUE, include=TRUE}
(prop.table(table(rf_train_data$Class))*100)

```


```{r testing features, echo = TRUE, include=TRUE}
rf_test_data_classCount <- rf_test_data |> 
  dplyr::group_by(Class) |> 
  count()

rf_test_data_classCount

```
Proportion of the response Classes that makes up the training data.

```{r testing feature proportions, echo = TRUE, include=TRUE}
(prop.table(table(rf_test_data$Class))*100)


```
The function trainControl generates parameters that further control how models are created

```{r setting the RF parameters, echo = TRUE, include=TRUE}


set.seed(124)
cvControl <- caret::trainControl(method = 'cv',
                                 number = 10,
                                 savePredictions = TRUE,
                                 verboseIter = FALSE)


respVar <- c("Class")
predVar <- c("Blue","Green","Red","RE","NIR","NDVI","EVI","SAVI")


rfModel <- caret::train(rf_train_data[,predVar],
                        rf_train_data[,respVar],
                        method="rf",
                        metric = "Kappa",
                        ntree= 500,
                        trControl= cvControl,
                        tuneLength=6,
                        importance=TRUE)


rfModel

```


```{r predicting, echo = TRUE, include=TRUE}

rfPredict <- predict(rfModel,rf_test_data)
head(rfPredict)

confusionMatrix(rfPredict,rf_test_data$Class)


```

```{r classifying, echo = TRUE, include=TRUE}

bawkuPredictions <- raster::predict(train_features_clnd_indexes, rfModel) 

```
```{r plotting, echo = TRUE, include=TRUE}



#Lets define the plalette(mainly be using Hexadecimal colours).
pal <-c("#7b6f89", "#e5ca81", "#81a581")

tm_shape(bawkuPredictions)+
  tm_raster(style = "cat",labels = c("bare_soil",
                                     "grass",
                                     "trees"),
            palette = pal,
            title = "Legend")+
  tm_layout(main.title= "",
            main.title.size =.9 )+
  tm_layout(legend.outside = TRUE,
            legend.outside.position = "right",
            legend.title.size = .8)









```


[Post Cover Image Credits](https://ecce.esri.ca/mac-blog/2019/06/03/random-forest-classification-with-r-and-collector-for-arcgis/)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

<!-- adding share buttons on the right side of the page -->
<!-- AddToAny BEGIN -->
<div class="a2a_kit a2a_kit_size_32 a2a_floating_style a2a_vertical_style" style="right:0px; top:150px; data-a2a-url="https://souzayuri.github.io/" data-a2a-title="Yuri Souza">
<a class="a2a_button_twitter"></a>
</div>
<script>
var a2a_config = a2a_config || {};
a2a_config.onclick = 1;
</script>
<script async src="https://static.addtoany.com/menu/page.js"></script>
<!-- AddToAny END -->