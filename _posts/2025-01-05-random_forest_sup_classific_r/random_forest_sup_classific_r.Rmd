---
title: "Supervised Image Classification with Random Forest in R"
description: |
  In this post, I demonstrate how to load and visualize georeferenced spatial data and perform a supervised random forest classification using R.
categories:
  - Machine Learning 
  - GIS
  - R
author:
  - name: Yuri Souza
date: 2025-03-21
preview: RF_example.png
output:
  distill::distill_article:
    self_contained: false

---
## **Quick intro and backgroung**

Machine learning has been one of my favorite topics over the last few years, as I have been using approaches such as predictive models and cluster analysis. However, these are just a subset of the machine learning potential. With that in mind, I decided to take a step forward and learn what I believe to be the next topic in my field of expertise: Supervised Image Classification. This is a highly effective method for automatically identifying features in an image, such as forest patches, crops, urban areas, water bodies, and so on. In a few words, the way it works is by telling an algorithm how each of these features looks like and then it search in the images for patterns on pixel aggregation that resemble the patterns of the input feature.    

I've been trying to perform supervised image classification of satellite images in R for a while, and I've always stumbled upon conceptual problems, such as: How to do it? Which parameters to use for validation? Which packages and functions? 

Even though this has been a very popular technique across remote sensing people and is widely available to learn on the internet, it is definitely easier to find tutorials for Python or software such as ArcGIS and QGIS than it is for R. I'm not saying there aren't tutorials in R out there, in fact, the content available here is from those tutorials, but I found this information very spread across a few blogs and a bit scarce.
So, after struggling to find a tutorial that I liked, I decided to create my own tutorial, based on what I could find online, while also adding my own touch. 

This is my attempt to learn how to perform supervised image classification, using the random forest algorithm. I'm pretty sure there are many other ways to carry it out, and definitely more efficient ones, but this is what I could get so far and I believe that will suite a large audience. 

For the record, this tutorial is a compilation of what is available in following blogs and tutorials:

- [WUR Geoscripting](https://geoscripting-wur.github.io/AdvancedRasterAnalysis/)

- [Cornell University](https://www.css.cornell.edu/faculty/dgr2/_static/files/R_html/CompareRandomForestPackages.html)

- [University of Cincinnati](https://uc-r.github.io/random_forests)

- [RPubs](https://rpubs.com/ials2un/rf_landcover)

- [RPubs](https://www.rpubs.com/BenKwesie/792661)

- [R-bloggers](https://www.r-bloggers.com/2021/04/random-forest-in-r/)

- [R Spatial](https://rspatial.org/rs/5-supclassification.html)

- [Valentin Stefan](https://valentinitnelav.github.io/satellite-image-classification-r/)

- [Urban Spatial](https://urbanspatial.github.io/classifying_satellite_imagery_in_R/)


## **That said, lets get started!!!**

In a few words, my goal here is to use a drone optical imagery I took during my fieldwork in Brazil and make a **model able to identify what is Bare Soil, Grass, and Trees** on it. This image was taken in *Chapada Dos Veadeiros National Park*, Brazil.

I broke down the workflow of this tutorial into six main steps, along with some intermediate ones, a #BONUS . This step-by-step approach really helped me grasp how the classification process works.

## [1. Installing and Loading the Packages](#section1)

## [2. Getting the Data Ready Getting the Data Ready](#section2)
### [2.1.](#section2-1) Loading the Datasets
### [2.2.](#section2-2) Checking up the Coordinate Reference System
### [2.3.](#section2-3) Checking up the Data Information and Structure
### [2.4.](#section2-4) Cropping the Raster Image
### [2.5.](#section2-5) Renaming and Selecting the Bands
### [2.6.](#section2-6) Visualizing the Raster and Polygons

## [3. Training the Model](#section3)
### [3.1.](#section3-1) Creating Vegetation Indexes
## [4. Evaluating the Model](#section4)

## [5. Classifying the Image](#section5)

## [6. Exploring the Results](#section6)

## [7. BONUS](#section7)


If you want to follow and try each step described here by yourself please download the data. [HERE](https://github.com/souzayuri/souzayuri.github.io/tree/main/_posts/2025-01-05-random_forest_sup_classific_r).
If you want to 


### **Hands to Work**

To get started, the very first step is to load the necessary packages (assuming you already have R installed on your machine). We'll utilize **raster** and **terra** for handling raster data, **sf** for managing spatial data like points and polygons, and **tidyverse** for data tabulation and visualization. Additionally, we'll rely on **caret** for implementing random forest methods, and use **mapview** along with **leaflet** to plot and visualize both the polygons and raster images. While these are the primary packages we'll need, I'll also incorporate some others to further explore the results of our image classification.


## 1. Installing and Loading the Packages{#section1}

Begin by installing and loading the necessary packages, along with the additional ones you will need. I prefer a setup this syntax that first checks if a package is already installed; if it isn't, it installs the package and then loads it afterward.

```{r packages, size = "tiny", include=TRUE}

if (!require("tidyverse")) install.packages("tidyverse")
if (!require("terra")) install.packages("terra")
if (!require("raster")) install.packages("raster")
if (!require("sf")) install.packages("sf")
if (!require("sp")) install.packages("sp")
if (!require("caret")) install.packages("caret")
if (!require("mapview")) install.packages("mapview")
if (!require("leaflet")) install.packages("leaflet")
if (!require("ggridges")) install.packages("ggridges")
if (!require("randomForest")) install.packages("randomForest")
if (!require("DiagrammeR")) install.packages("DiagrammeR")
if (!require("tmap")) install.packages("tmap")
if (!require("leafem")) install.packages("leafem")
if (!require("plotly")) install.packages("plotly")


```
## 2. Getting the Data Ready{#section2}

### **2.1 Loading the datasets**{#section2-1}

Let's load all the files and information we will need later on:

- **woody_savanna.tif**: this is a drone image that I took during my master's. It has 6 stacked bands: Blue, Green, Red, Red Edge, Near Infrared, and a Mask in this respective order. The mask band represents pixels that could not be identified during the image compilation (to make this image, I needed to use Agisoft Metashape).

- **sampled_area.shp**: this is a polygon shapefile that we will use to clip the image and restrict it to a area of 100m x 100m (therefore removing the edges of the image).

- **train_features.shp**: this file contains the features we will use to train our Random Forest model. It contains three classes: Bare Soil, Grass, and Trees.

In this step we load the files and make sure to rename their classes using a standard nomenclature.

```{r loading and reading the files, size = "small", include=TRUE}

raw_raster <- terra::rast("woody_savanna.tif") |> 
  terra::aggregate(fact=2) # original pixel size = 10.8cm, new pixel size 21.6cm (old * 2)
raw_raster

plot(raw_raster)

sampled_area <- terra::vect("sampled_area.shp")
sampled_area
plot(sampled_area)


train_features <- sf::st_read("train_features.shp")
train_features

# renaming their classes accordingly to keep it consistent across upcoming analyses.
train_features_clnd <- sf::st_read("train_features.shp") |> 
  dplyr::rename(OBJECTID = "Classcode",
                Class = "Classname") |> 
  dplyr::mutate(OBJECTID = as.character(1:46),
                Class = as.factor(Class),
                Code = as.numeric(Class)) |> 
  dplyr::select(c(1:3))

train_features_clnd
plot(train_features_clnd)
```

### **2.2. Checking up the Coordinate Reference System**{#section2-2}

We also check their CRS (coordinate reference system) and ensure they all have the same coordinate properties.

```{r checking up the CRS, size = "small", include=TRUE}

# Compare CRS, if they are different make them equal
if (terra::crs(raw_raster) == terra::crs(train_features_clnd)) {
  print("CRS are identical!")
} else {
  print("CRS are different. Therefore, reprojecting to match raster projection...")
  # Reproject train_features to match raw_raster
  train_features_clnd <- sf::st_transform(train_features_clnd, crs = terra::crs(raw_raster))
}


# Compare CRS again to ensure they above function worked.
if (terra::crs(raw_raster) == terra::crs(train_features_clnd)) {
  print("CRS are identical!")
} else {
  print("CRS are different. Therefore, reprojecting to match raster projection...")
  # Reproject train_features to match raw_raster
  train_features_clnd <- sf::st_transform(train_features_clnd, crs = terra::crs(raw_raster))
}


```

### **2.3. Checking up the data information and structure**{#section2-3}

Let's check the training data and the number of features, which has a total of 46. 
Here we can see that every training class has 15 features/polygons (grass has 16 because I added one more by mistake).

```{r checking up the data, size = "small", include=TRUE}
glimpse(train_features_clnd)
summary(train_features_clnd)

```

### **2.4. Cropping the raster image**{#section2-4}

Now that we have standardized the data, we will crop the raster image to the 100m x 100m grid limit. 
We also want to check the CRS and make sure it is correct, and plot each of these bands side-by-side.
 
```{r cropping the raster image, size = "small", include=TRUE}

cropped_raster <- terra::crop(raw_raster, sampled_area)
cropped_raster
plot(cropped_raster)


# Compare CRS
if (terra::crs(raw_raster) == terra::crs(train_features_clnd)) {
  print("CRS are identical!")
} else {
  print("CRS are different. Therefore, reprojecting to match raster projection...")
  # Reproject train_features to match raw_raster
  train_features_clnd <- terra::project(train_features_clnd, crs(raw_raster))
}


```
### **2.5. Renaming and Selecting the Bands**{#section2-5}

So far, we have ensured the consistency of the image coordinates and defined the grid area we want to work on.
Now, let's rename the bands according to their respective spectral signatures. This will help us later identify which band is which.
In this step, I also removed the band MASK by only selecting the bands of interest according to their position in the bands list (1 to 5).

```{r defining band names, size = "small", include=TRUE}

names(cropped_raster) <- c("Blue","Green","Red","RE","NIR","MASK")
plot(cropped_raster)

cropped_raster_bands <- cropped_raster[[1:5]]
plot(cropped_raster_bands)

```
### **2.6. Visualizing the Raster and Polygons**{#section2-6}

Now that we have the raster with five bands and the polygons ready for training the model, let's visualize our data. We'll create a natural color composition. To do this we need to inform the position of the RGB bands of the raster image (that is why naming it before hand was important) and overlay it with each class represented by the polygons. For visualization, we will use mapview() because it's interactive (just because we want some cool!!!).

```{r visualizing the raster and the polygons, echo = TRUE, include=TRUE}

train_class_colors <- unique(train_features_clnd$Class) |> 
  as.data.frame() |>
  dplyr::mutate(hex = c(grass = "#ff7f00",
                        bare_soil = "#e41a1c",
                        tree = "#55eed1"))
train_class_colors

mapview::viewRGB(brick(cropped_raster_bands[[1:3]]), 
                 r = 3, g = 2, b = 1,
                 map.types = c("Esri.WorldImagery", "OpenStreetMap", "OpenTopoMap"),
                 maxpixels =  866760,
                 layer.name = "RBG Image") + 
  mapview::mapView(train_features_clnd, 
                   zcol = "Class", 
                   color = "black",  
                   lwd = 2,
                   col.regions = train_class_colors$hex,
                   layer.name = "Features")


```

## 3. Training the Model{#section3}

One cool aspect of using optical imagery rasters with various bands is the ability to rearrange these bands to create new, informative images. To provide some context, the drone imagery we're working with includes natural color RGB bands, which are part of the visible spectrum for humans, along with the Red Edge and Near-Infrared bands, which lie outside this spectrum. The latter two bands are particularly useful for highlighting vegetation characteristics, as plants reflect a significant amount of infrared light. This unique property makes infrared imagery an invaluable tool for analyzing vegetation, as it offers deeper insights into the state, health, vigor, and other attributes of plants that natural color imagery simply cannot reveal.

Now that we have our raster image and training features prepared, let's move on to the next step: using this raster image and bands to create various vegetation indexes. They will be stacked as bands and will be used to train the model, using the mapped polygons, and to predict the vegetation classes in the later steps of the classification. This process will improve the classification of the image by pinpointing pixel combinations that are more accurately representative of the specific vegetation characteristics we aim to map.


### **3.1. Creating Vegetation Indexes**{#section3-1}

When conducting image classification, it’s crucial to provide clear information that highlights the distinguishing features you’re mapping. In this section, we will focus on creating three widely used vegetation indexes in remote sensing: the NDVI (Normalized Difference Vegetation Index), the EVI (Enhanced Vegetation Index), and the SAVI (Soil Adjusted Vegetation Index). Each of these indexes interprets vegetation in unique ways, and we will utilize them alongside the spectral bands from the drone imagery to classify the image effectively.

This is a straightforward process that primarily involves utilizing the Red, Near Infrared, and Blue bands, along with a few predefined parameters. Each index has its own arithmetic equation, and we can identify these bands based on their positions within the raster. We will then plot these indexes side-by-side.

Check out below a table summarizing the similarity and differences among these indexes.

| Feature           | **NDVI** (Normalized Difference Vegetation Index)       | **EVI** (Enhanced Vegetation Index)                                 | **SAVI** (Soil Adjusted Vegetation Index)                           |
|-------------------|---------------------------------------------------------|----------------------------------------------------------------------|----------------------------------------------------------------------|
| **Formula**        | (NIR - Red) / (NIR + Red)                                | 2.5 × (NIR - Red) / (NIR + 6 × Red - 7.5 × Blue + 1)                   | ((NIR - Red) / (NIR + Red + 0.5)) × (1 + 0.5)                            |
| **Bands Used**     | NIR, Red                                                | NIR, Red, Blue                                                       | NIR, Red                                                             |
| **Purpose**        | General greenness/vegetation presence                   | Improves sensitivity in high biomass areas                           | Adjusts for soil background noise                                   |
| **Advantages**     | Simple, widely used                                     | Corrects for atmospheric & canopy background effects                 | Better for areas with sparse vegetation or exposed soil             |
| **Limitations**    | Sensitive to soil brightness and atmospheric conditions | More complex, needs Blue band                                        | Requires adjustment factor (L) based on soil type                   |                         |
| **Value Range**    | -1 to +1 (usually 0 to 0.8 in vegetated areas)          | -1 to +1 (typically 0 to 0.9 for vegetation)                         | -1 to +1 (usually 0 to 0.7, depending on L and vegetation density)  |



```{r creating vegetation indexes, echo = TRUE, include=TRUE}

nir <- cropped_raster_bands[[5]]
red <- cropped_raster_bands[[3]]
blue <- cropped_raster_bands[[1]]

# Check the range of your input bands
print(paste("NIR range:", minmax(nir)))
print(paste("Red range:", minmax(red)))
print(paste("Blue range:", minmax(blue)))

cropped_raster_bands_ndvi <- (nir - red) / (nir + red)
cropped_raster_bands_ndvi

cropped_raster_bands_evi <- 2.5 * ((nir - red) / (nir + (6 * red) - (7.5 * blue) + 1))
cropped_raster_bands_evi

cropped_raster_bands_savi <- ((nir - red) / (nir + red + 0.5)) * (1 + 0.5)
cropped_raster_bands_savi

# Create a red-to-green color palette
red_to_green_palette <- colorRampPalette(c("darkred", "yellow", "darkgreen"))
# Lets visuallize them together
par(mfrow = c(1, 3))

plot(cropped_raster_bands_ndvi, main = "NDVI", col = red_to_green_palette(100))
plot(cropped_raster_bands_evi, main = "EVI", col = red_to_green_palette(100))
plot(cropped_raster_bands_savi, main = "SAVI", col = red_to_green_palette(100))

par(mfrow = c(1, 1))


```
Let's stack these three indexes in our raster data so we can have more information to feed the model. After stacking the rasters containing the indexes with our main raster, we will notice that the new rasters are named as NIR. That is because the first spectral band we used in the above calculations was the NIR band. To avoid confusion between the NIR spectral band and the new rasters indexes, we will name them according to which index they represent.

```{r visualizing datax, echo = TRUE, include=TRUE}

train_features_clnd_indexes <- c(cropped_raster_bands, 
                                 cropped_raster_bands_ndvi,
                                 cropped_raster_bands_evi, 
                                 cropped_raster_bands_savi)
train_features_clnd_indexes


names(train_features_clnd_indexes) <- c(names(cropped_raster_bands),"NDVI", "EVI", "SAVI")
train_features_clnd_indexes

plot(train_features_clnd_indexes)

```


Now we have everything to move forward to prepare the data that will be use for training and testing our RF model. 
Lets get started by extracting the pixel values for those features we mapped. These pixel values will be used to train and test the model later.

```{r visualizing datas, echo = TRUE, include=TRUE}

train_features_clnd_pxls <- extract(train_features_clnd_indexes, train_features_clnd)
head(train_features_clnd_pxls)


train_features_clnd_pxls$Class <- train_features_clnd$Class[train_features_clnd_pxls$ID]
head(train_features_clnd_pxls)

train_features_clnd_pxls$ID <- NULL

summary(train_features_clnd_pxls)

```
Now that we have given those polygons delimitation pixel information, lets take a look at some of these pixels distribution

```{r visualizing indexes histogram, echo = TRUE, include=TRUE}
val_grass <- subset(train_features_clnd_pxls, Class == "grass")
head(val_grass)
val_trees <- subset(train_features_clnd_pxls, Class == "trees")
head(val_trees)
val_bare_soil <- subset(train_features_clnd_pxls, Class == "bare_soil")
head(val_bare_soil)

# NDVI
par(mfrow = c(3, 1))
hist(val_grass$NDVI, main = "grass", xlab = "NDVI", xlim = c(0, 1), col = "#e5ca81")
hist(val_trees$NDVI, main = "trees", xlab = "NDVI", xlim = c(0, 1), col = "#81a581")
hist(val_bare_soil$NDVI, main = "bare_soil", xlab = "NDVI", xlim = c(0, 1), col = "#7b6f89")
```


```{r visualizing indexes effects, echo = TRUE, include=TRUE}
par(mfrow = c(1, 1))

# Scatterplot and trend lines of NIR and SAVI bands 
plot(NDVI ~ EVI, data = val_grass, pch = ".", col = "#e5ca81", xlab = "SAVI", ylab = "NDVI")
points(NDVI ~ EVI, data = val_trees, pch = ".", col = "#81a581")
points(NDVI ~ EVI, data = val_bare_soil, pch = ".", col = "#7b6f89")


model_grass <- lm(NDVI ~ EVI, data = val_grass) 
abline(model_grass, col = "#e5ca81", lwd = 2)    

model_trees <- lm(NDVI ~ EVI, data = val_trees) 
abline(model_trees, col = "#81a581", lwd = 2)   


model_bare_soil <- lm(NDVI ~ EVI, data = val_bare_soil) 
abline(model_bare_soil, col = "#7b6f89", lwd = 2)        

legend("topright", legend = c("grass", "trees", "bare_soil"), 
       fill = c("#e5ca81", "#7b6f89", "#81a581"), bg = "white")


```
Lets visualize the three indexes for each mapped feature in a single graph

```{r visualizing and comparing indexes distribution using histograms, echo = TRUE, include=TRUE}

train_features_clnd_pxls_indexes <- train_features_clnd_pxls |> 
  dplyr::select(c(NDVI, EVI, SAVI, Class)) |> 
  tidyr::pivot_longer(cols = 1:3, values_to = "values", names_to = "indexes")
train_features_clnd_pxls_indexes


# add the distribution curve for each indexes parameter using stat_function and dnorm function
ggplot(train_features_clnd_pxls_indexes, 
                        aes(x = values, 
                            fill = indexes)) +
  geom_density_ridges(aes(y = 0, fill = indexes, color = indexes),  
                      jittered_points = TRUE, 
                      alpha = .5, 
                      scale = 1,
                      point_shape = "|",
                      point_size = 3, 
                      position = ggridges::position_points_jitter(width = 0.05, height = 0)) +  
    geom_vline(data = train_features_clnd_pxls_indexes |> 
               dplyr::group_by(Class, indexes) |> 
               dplyr::summarise(mean = mean(values), .groups = "drop") |> 
               dplyr::ungroup(),
             aes(xintercept = mean, color = indexes), linetype = "dashed", size = 0.8) +
  scale_fill_manual(values = c("#e5ca81", "#7b6f89", "#81a581"), labels = c("EVI", "NDVI", "SAVI")) +
  scale_color_manual(values = c("#e5ca81", "#7b6f89", "#81a581"), labels = c("EVI", "NDVI", "SAVI")) +
  theme_bw() + labs(x = "", y = "Frequency") +
  theme(axis.title = element_text(size = 20, face = "bold", color = "gray20"),
        axis.text.x.bottom = element_text(size = 14),
        axis.text.y = element_text(size = 14),
        legend.direction = "horizontal",
        legend.position = "bottom",
        legend.title = element_text(size = 16, face = "bold"),
        legend.text = element_text(size = 14),
        title = element_text(size = 18)) +
  facet_wrap(~Class, ncol = 3, scales = "free")


```

Now lets build the train and test data. At this stage we partition our dataset into two, thats a train and test dataset. 
We train the model with the train dataset and evaluate the model perfomance using the test data.

We would use the caret function createDataPartition() to split data into training and testing sets.

```{r partitioning the data, echo = TRUE, include=TRUE}
set.seed(124) 

str(train_features_clnd_pxls)

# partitioning the data into 80%
rf_train_features_clnd_pxls_indexes <- caret::createDataPartition(train_features_clnd_pxls$Class, list = FALSE, p = 0.8)
```

Now that we have partitioned 80% of the dataset, we will select that portion of that data to be used as train data (80%) and test data (20%)
If we check the number of rows in the new datasets and use the "rule of three" equation we can confirm that the data was properly split just as asked. 

```{r extracting the data to be used as train and test, echo = TRUE, include=TRUE}
# setting 80% for training - Use rf_train_features_clnd_pxls_indexes as a vector of row indices that specify which rows to extract from the train_features_clnd_pxls_indexes dataset.
rf_train_data <- train_features_clnd_pxls[rf_train_features_clnd_pxls_indexes, ] 
str(rf_train_data)

# setting 20% for training - Use rf_train_features_clnd_pxls_indexes as a vector of row indices that specify which rows to extract from the train_features_clnd_pxls_indexes dataset.
rf_test_data <- train_features_clnd_pxls[-rf_train_features_clnd_pxls_indexes, ]
str(rf_test_data)

```

Lets check the proportion of features selected by each set of data

```{r training features, echo = TRUE, include=TRUE}
rf_train_data_classCount <- rf_train_data |> 
  dplyr::group_by(Class) |> 
  count()

rf_train_data_classCount

```

```{r training feature proportions, echo = TRUE, include=TRUE}
(prop.table(table(rf_train_data$Class))*100)

```

```{r testing features, echo = TRUE, include=TRUE}
rf_test_data_classCount <- rf_test_data |> 
  dplyr::group_by(Class) |> 
  count()

rf_test_data_classCount

```
Proportion of the response Classes that makes up the training data.

```{r testing feature proportions, echo = TRUE, include=TRUE}
(prop.table(table(rf_test_data$Class))*100)


```
The function trainControl generates parameters that further control how models are created

```{r setting the RF parameters, echo = TRUE, include=TRUE}
set.seed(124)
cvControl <- caret::trainControl(method = 'cv',
                                 number = 10,
                                 savePredictions = TRUE,
                                 verboseIter = FALSE)


respVar <- c("Class")
predVar <- c("Blue","Green","Red","RE","NIR","NDVI","EVI","SAVI")


rfModel <- caret::train(rf_train_data[,predVar],
                        rf_train_data[,respVar],
                        method="rf",
                        metric = "Kappa",
                        ntree= 1000,
                        trControl= cvControl,
                        tuneLength=6,
                        importance=TRUE)

rfModel

```

```{r plot model, echo = TRUE, include=TRUE}

plot(rfModel)

```
```{r var error, echo = TRUE, include=TRUE}

plot(rfModel$finalModel)


```

```{r var contribution, echo = TRUE, include=TRUE}

varImpPlot(rfModel$finalModel)


```

```{r predicting, echo = TRUE, include=TRUE}

rfPredict <- predict(rfModel,rf_test_data)
head(rfPredict)

conf_matrix <- confusionMatrix(rfPredict, rf_test_data$Class)
conf_matrix

conf_table <- as.data.frame(conf_matrix$table)

colnames(conf_table) <- c("Predicted", "Actual", "Frequency")
conf_table

ggplot(data = conf_table, aes(x = Actual, y = Predicted, fill = Frequency)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "white", high = "darkgreen") +
  geom_text(aes(label = Frequency), color = "black", size = 5) +
  labs(title = "Confusion Matrix Heatmap",
       x = "Actual Labels",
       y = "Predicted Labels",
       fill = "Frequency") +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

```{r classifying, echo = TRUE, include=TRUE}

image_lc_predict <- raster::predict(train_features_clnd_indexes, rfModel) 

```


```{r plotting, echo = TRUE, include=TRUE}

#Lets define the plalette(mainly be using Hexadecimal colours).
pal <-c("#7b6f89", "#e5ca81", "#81a581")

tm_shape(image_lc_predict)+
  tm_raster(style = "cat",labels = c("bare_soil",
                                     "grass",
                                     "trees"),
            palette = pal,
            title = "Legend")+
  tm_layout(main.title= "",
            main.title.size =.9 )+
  tm_layout(legend.outside = TRUE,
            legend.outside.position = "right",
            legend.title.size = .8)


```

```{r plotting interactively, echo = TRUE, include=TRUE}

raster_lc_predict <- raster(image_lc_predict)

# Define the palette for the classified raster
pal <- colorFactor(
  palette = c("#7b6f89", "#e5ca81", "#81a581"), 
  levels = c(1, 2, 3),
  na.color = "transparent" 
)


leaflet() |> 
  addTiles(group = "Base Map") |> 
  addRasterRGB(brick(cropped_raster_bands[[1:3]]),
               r = 3, g = 2, b = 1,
               group = "RGB Image") |> 
  addRasterImage(raster_lc_predict,
                 colors = pal,
                 opacity = 0.4,
                 group = "Land Cover Classification") |> 
  addLegend(pal = pal,
            values = c(1, 2, 3), 
            title = "Land Cover Classification",
            labels = c("bare_soil", "grass", "trees"),
            position = "bottomright") |> 
    addLayersControl(
    baseGroups = c("Base Map", "RGB Image"),
    overlayGroups = c("Land Cover Classification"),
    options = layersControlOptions(collapsed = FALSE))

```

``` {r visualizing pixel frequency, echo = TRUE, include=TRUE}


raster_lc_predict_df <- as.data.frame(raster_lc_predict) |> 
  mutate(category = case_when(
    class == 1 ~ "bare_soil",
    class == 2 ~ "grass",
    class == 3 ~ "trees",
    TRUE ~ NA_character_)) |> 
  group_by(category) |> 
  summarise(count = n()) |> 
  ungroup() |> 
  mutate(prop = round(((count*100)/(sum(count))), 2)) |> 
  arrange(desc(prop))

head(raster_lc_predict_df)


plot_ly(raster_lc_predict_df, 
        x = ~category, 
        y = ~prop,
        type = "bar",
        text = ~prop, 
        textposition = 'auto') |> 
        layout(title = "Vegetation Cover",
               xaxis = list(title = "Vegetation Type",
                            tickvals = c("bare_soil", "grass", "trees"),
                            ticktext = c("Bare Soil", "Grass", "Trees")),
               yaxis = list(title = "Cover Proportation (%)")) 

```


[Post Cover Image Credits](https://ecce.esri.ca/mac-blog/2019/06/03/random-forest-classification-with-r-and-collector-for-arcgis/)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)


```

<!-- adding share buttons on the right side of the page -->
<!-- AddToAny BEGIN -->
<div class="a2a_kit a2a_kit_size_32 a2a_floating_style a2a_vertical_style" style="right:0px; top:150px; data-a2a-url="https://souzayuri.github.io/" data-a2a-title="Yuri Souza">
<a class="a2a_button_twitter"></a>
</div>
<script>
var a2a_config = a2a_config || {};
a2a_config.onclick = 1;
</script>
<script async src="https://static.addtoany.com/menu/page.js"></script>
<!-- AddToAny END -->